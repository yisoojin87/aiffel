{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "running-graph",
   "metadata": {},
   "source": [
    "# Exploration 4. 멋진 작사가 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "psychological-register",
   "metadata": {},
   "source": [
    "### Step1. 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-causing",
   "metadata": {},
   "source": [
    "#### ~/aiffel/lyricist/data/lyrics 에 데이터가 이미 존재한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outstanding-hawaii",
   "metadata": {},
   "source": [
    "### Step2. 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "czech-announcement",
   "metadata": {},
   "source": [
    "##### 자연어 데이터를 코퍼스(corpus)라고 부르며 특정 도메인으로부터 수집된 텍스트 집합을 말한다.\n",
    "##### 텍스트 데이터의 파일형식은 txt, csv, xml파일 등으로 다양한데 이번 프로젝트에서는 txt 파일을 가져와 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abroad-infrastructure",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Example: \n",
      " [' There must be some kind of way outta here', 'Said the joker to the thief', \"There's too much confusion\"]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os, re\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "txt_file_path = os.getenv('HOME') + '/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "# 여러개의 txt파일을 모두 읽어서 raw_corpus에 담는다.\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "        \n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Example: \\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-blond",
   "metadata": {},
   "source": [
    "### Step3. 데이터 정제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-sacramento",
   "metadata": {},
   "source": [
    "##### 자연어 처리에 있어서 텍스트를 용도에 맞게 사전에 처리하는 작업인 텍스트 전처리가 매우 중요한 작업이다.\n",
    "##### 정규 표현식에서 re.sub()함수를 사용하여 정규 표현식과 패턴과 일치하는 문자열을 찾아 다른 문자열로 대체할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mounted-longer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> none were level on the mind <end>\n"
     ]
    }
   ],
   "source": [
    "# 정규표현식(Regex)\n",
    "\n",
    "# 입력된 문장을\n",
    "#  1. 소문자로 바꾸고, 양쪽 공백을 지운다.\n",
    "#  2. 특수문자 양쪽에 공백을 넣고\n",
    "#  3. 여러개의 공백은 하나의 공백으로 바꾼다.\n",
    "#  4. a-zA-Z?.!가 아닌 모든 문자를 하나의 공백으로 바꾼다.\n",
    "#  5. 다시 양쪽 공백을 지웁니다.\n",
    "#  6. 문장 시작에는 <start>, 끝에는 <end>를 추가한다.\n",
    "# 위의 순서대로 처리해주면 문제가 되는 상황을 방지할 수 있다.\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4 (이미 숫자도 제외되고 있음. ^기호가 대괄호 안에 있으면 대괄호 안에 있는 문자 제외하고 공백으로 처리)\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(raw_corpus[5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "connected-registration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> there must be some kind of way outta here <end>',\n",
       " '<start> said the joker to the thief <end>',\n",
       " '<start> there s too much confusion <end>',\n",
       " '<start> i can t get no relief business men , they drink my wine <end>',\n",
       " '<start> plowman dig my earth <end>',\n",
       " '<start> none were level on the mind <end>',\n",
       " '<start> nobody up at his word <end>',\n",
       " '<start> hey , hey no reason to get excited <end>',\n",
       " '<start> the thief he kindly spoke <end>',\n",
       " '<start> there are many here among us <end>']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 여기에 정제된 문장을 모읍시다\n",
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    # 우리가 원하지 않는 문장은 건너뜁니다\n",
    "    # 정제를 하고 담아주세요\n",
    "    \n",
    "    if len(sentence) == 0: continue\n",
    "    tmp = preprocess_sentence(sentence)\n",
    "    if len(tmp.split()) > 15: continue     # 토큰의 개수가 15개를 넘어가는 문장을 학습데이터에서 제외\n",
    "    corpus.append(tmp)\n",
    "        \n",
    "# 정제된 결과를 10개만 확인해보죠\n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-average",
   "metadata": {},
   "source": [
    "##### 케라스(Keras)는 기본적인 전처리를 위한 도구들을 제공한다. \n",
    "##### tokenizer.fit_on_texts() 안에 코퍼스를 입력으로 하면 빈도수를 기준으로 단어 집합을 생성한다.\n",
    "##### tokenizer.texts_to_sequences()로 텍스트 시퀀스의 모든 단어들을 각 정수로 맵핑한다.\n",
    "##### 문장의 길이가 각각 다 다르기 때문에 여러 문장의 길이를 임의로 동일하게 맞춰주는 작업을 패딩(padding)이라고 한다.\n",
    "##### 문장의 길이를 가장 긴 문장에 맞추는 것이 아닌 maxlen=15로 인자를 정해서 모든 문서의 길이를 동일하게 맞춰준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "above-process",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2  62 271 ...   0   0   0]\n",
      " [  2 117   6 ...   0   0   0]\n",
      " [  2  62  17 ...   0   0   0]\n",
      " ...\n",
      " [  2  75  45 ...   3   0   0]\n",
      " [  2  49   5 ...   0   0   0]\n",
      " [  2  13 635 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f9f15cb1b50>\n"
     ]
    }
   ],
   "source": [
    "# 토큰화할 때 텐서플로우의 Tokenizer와 pad_sequences를 사용한다.\n",
    "\n",
    "def tokenize(corpus):\n",
    "    # 12000단어를 기억할 수 있는 tokenizer를 만들겁니다\n",
    "    # 우리는 이미 문장을 정제했으니 filters가 필요없어요\n",
    "    # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿀거에요\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000, \n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "    # corpus를 이용해 tokenizer 내부의 단어장을 완성합니다\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    # 준비한 tokenizer를 이용해 corpus를 Tensor로 변환합니다\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    # 입력 데이터의 시퀀스 길이를 일정하게 맞춰줍니다\n",
    "    # 만약 시퀀스가 짧다면 문장 뒤에 패딩을 붙여 길이를 맞춰줍니다.\n",
    "    # 문장 앞에 패딩을 붙여 길이를 맞추고 싶다면 padding='pre'를 사용합니다\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=15)  # pre-padding을 적용할 경우 성능이 더 좋아진다~!!!뒤로 가면 갈수록 데이터가 더 중요하기 때문에\n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranging-designer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   62  271   27   94  546   20   86  743   90]\n",
      " [   2  117    6 6269   10    6 2310    3    0    0]\n",
      " [   2   62   17  102  184 2718    3    0    0    0]]\n"
     ]
    }
   ],
   "source": [
    "# 생성된 텐서 데이터를 3번째 행, 10번째 열까지만 \n",
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cordless-customer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : i\n",
      "5 : ,\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "# tokenizer에 구축된 단어 사전이 어떻게 구축되었는지 확인\n",
    "\n",
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "threatened-cooler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2  62 271  27  94 546  20  86 743  90   3   0   0   0]\n",
      "[ 62 271  27  94 546  20  86 743  90   3   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성한다.\n",
    "# 마지막 토큰은 <end>가 아니라 <pad>일 가능성이 높다.\n",
    "src_input = tensor[:, :-1]\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성한다.\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-folder",
   "metadata": {},
   "source": [
    "### Step4. 평가 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "moving-contemporary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터셋 객체 생성\n",
    "\n",
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "# tokenizer가 구축한 단어사전 내 12000개와, 여기 포함되지 않은 0:<pad>를 포함하여 12001개\n",
    "VOCAB_SIZE = tokenizer.num_words + 1\n",
    "\n",
    "# 준비한 데이터 소스로부터 데이터셋을 만듦\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "through-suicide",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source Train: (124981, 14)\n",
      "Source Val: (31246, 14)\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터와 평가데이터 분리\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size = 0.2, random_state= 10)\n",
    "print(\"Source Train:\", enc_train.shape)\n",
    "print(\"Source Val:\", enc_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-dependence",
   "metadata": {},
   "source": [
    "### Step5. 인공지능 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "institutional-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개의 Embedding 레이어, 2개의 LSTM 레이어, 1개의 Dense 레이어로 구성한 구조도\n",
    "\n",
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 1024\n",
    "hidden_size = 3000\n",
    "model = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "preliminary-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3906/3906 [==============================] - 2019s 503ms/step - loss: 3.1878 - val_loss: 2.6402\n",
      "Epoch 2/5\n",
      "3906/3906 [==============================] - 1964s 503ms/step - loss: 2.4026 - val_loss: 2.3777\n",
      "Epoch 3/5\n",
      "3906/3906 [==============================] - 1964s 503ms/step - loss: 1.8861 - val_loss: 2.2380\n",
      "Epoch 4/5\n",
      "3906/3906 [==============================] - 1964s 503ms/step - loss: 1.4856 - val_loss: 2.1942\n",
      "Epoch 5/5\n",
      "3906/3906 [==============================] - 1964s 503ms/step - loss: 1.2356 - val_loss: 2.2221\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9f1591f210>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습시키기\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits = True,\n",
    "    reduction = 'none')\n",
    "\n",
    "model.compile(loss = loss, optimizer = optimizer)\n",
    "model.fit(enc_train, dec_train, epochs = 5, validation_data=(enc_val, dec_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "traditional-scanning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you , bye bye <end> '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가하기\n",
    "\n",
    "def generate_text(model, tokenizer, init_sentence = \"<start>\", max_len=20):\n",
    "    # 테스트를 위해서 입력받은 init_sentence도 텐서로 변환합니다.\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "    \n",
    "    # 단어 하나씩 예측해 문장을 만듭니다.\n",
    "    #  1. 입력받은 문장의 텐서를 입력합니다.\n",
    "    #  2. 예측된 값 중 가장 높을 확률인 word index를 뽑아냅니다.\n",
    "    #  3. 2에서 예측된 word index를 문장 뒤에 붙입니다.\n",
    "    #  4. 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마칩니다.\n",
    "    \n",
    "    while True:\n",
    "        # 1\n",
    "        predict = model(test_tensor)\n",
    "        # 2\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1]\n",
    "        # 3\n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        # 4\n",
    "        if predict_word.numpy()[0] == end_token:break\n",
    "        if test_tensor.shape[1] >= max_len:break\n",
    "            \n",
    "    generated = \"\"\n",
    "    # tokenizer를 이용해 word index를 단어로 하나씩 변환합니다.\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "        \n",
    "    return generated\n",
    "\n",
    "generate_text(model, tokenizer, init_sentence = \"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perceived-composer",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-recycling",
   "metadata": {},
   "source": [
    "1. NLP를 처음 접해보았는데 데이터를 정제하는 부분이 새롭기도 하고 언어를 사용하고자 하는 용도에 맞게 처리하는 것이 어려우면서도 중요한 작업인 걸 깨달았다.      \n",
    "     \n",
    "          \n",
    "2. NLP를 접하면서 아주 조금은 CNN과 RNN의 개념이 구분이 가기 시작했다. CNN은 feature(특징)을 추출하여 패턴을 파악하는 구조이며 RNN은 시퀀스 구조로 순차적으로 이루어진 데이터 처리에 적합하며 과거의 값이 현재의 값에 반영이 되는 구조이다.     \n",
    "      \n",
    "           \n",
    "3. val_loss를 2.2이하 수준으로 줄이기 위해 embedding_size를 256, 512, 1024로 변화를 주었고 hidden_size로 2000, 3000으로 변화를 주었지만 2.2 ~ 2.3 수준으로 크게 변화가 있지는 않았다.      \n",
    "     \n",
    "          \n",
    "4. TF 마스터 시간에 padding에 대해서 이야기를 나누었는데 가변적인 문장 길이를 같은 길이로 맞춰주기 위해서 사용된다는 것을 알게 되었고 뒤에서 채우는 post-padding보다는 앞에서부터 패딩을 채우는 pre-padding이 더 성능이 좋게 나올 것이라는 이야기를 나누었다. 그래서 실제로 padding의 default값이 'pre'로 되어있다는 이야기도 나누었다. 실제로 padding을 'pre'로 넣고 다시 코드를 실행해 보았지만 val_loss가 더 낮게 나올것이라는 내 예상과는 달리 그렇게 성능이 좋게 나오지는 않았다. (padding='pre'를 넣고 돌린 결과 val_loss가 2.2 ~ 2.3으로 padding='post'와 별반 다르지 않았다.)    \n",
    "      \n",
    "           \n",
    "5. epoch당 20~30분 정도 시간이 걸려 val_loss를 도출하기까지 꽤 오랜 시간이 걸려 많이 돌려보지 못하였다. 3번정도 코드를 실행해 보았을 때의 결과는 다음과 같이 나왔다. input으로 '<start> i love'만 입력하였는데 뒤에 자동으로 문장이 완성되는 것이 신기했다. 다음에 시간있을 때 다른 문장도 한 번 넣어서 만들어 봐야겠다.       \n",
    "         \n",
    "- 1회차 : 'i love you so much, i love you so much'        \n",
    "- 2회차 : 'i love the way you shake your thing'      \n",
    "- 3회차 : 'i love you, bye bye'      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
